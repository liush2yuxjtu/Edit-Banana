# Edit-Banana API ä¿®å¤å»ºè®®æŠ¥å‘Š

## 1. API æ›¿æ¢å¯è¡Œæ€§åˆ†æè¡¨

| åŸ API | ç”¨é€” | æ˜¯å¦å¯æ›¿æ¢ | æ›¿æ¢æ–¹æ¡ˆ | éš¾åº¦ | å¤‡æ³¨ |
|--------|------|-----------|---------|------|------|
| **Azure Document Intelligence** | æ–‡æœ¬å®šä½/OCR | âš ï¸ éƒ¨åˆ†å¯ | PaddleOCR / EasyOCR + Kimi æ ¡éªŒ | é«˜ | OCR éœ€è¦ä¸“ç”¨æ¨¡å‹ï¼ŒKimi æ— æ³•ç›´æ¥æ›¿ä»£ |
| **Azure OpenAI (GPT-4V)** | å›¾åƒç†è§£/åˆ†æ | âœ… æ˜¯ | Kimi è§†è§‰æ¨¡å‹ (kimi-v1) | ä½ | ç›´æ¥æ›¿æ¢ï¼ŒAPI æ ¼å¼å…¼å®¹ |
| **OpenAI GPT-4V** | å›¾åƒç†è§£ | âœ… æ˜¯ | Kimi è§†è§‰æ¨¡å‹ | ä½ | ç›´æ¥æ›¿æ¢ |
| **Mistral** | å…¬å¼è¯†åˆ«/LaTeX | âœ… æ˜¯ | Kimi æ–‡æœ¬æ¨¡å‹ | ä½ | Kimi æ”¯æŒ LaTeX è¾“å‡º |
| **Mistral** | æ–‡æœ¬ä¿®æ­£ | âœ… æ˜¯ | Kimi æ–‡æœ¬æ¨¡å‹ | ä½ | ç›´æ¥æ›¿æ¢ |

### Kimi API èƒ½åŠ›éªŒè¯

```bash
# Kimi API ç«¯ç‚¹
ANTHROPIC_BASE_URL=https://api.kimi.com/coding/
ANTHROPIC_API_KEY=sk-kimi-SckItPdArEsXNGKFoCYHMd8uG1FjpDnG8m1mEi1vQ6VMzhMhtVgFVqKilMthoVXN
```

**Kimi æ”¯æŒçš„èƒ½åŠ›ï¼š**
- âœ… æ–‡æœ¬ç”Ÿæˆï¼ˆä¸ Claude å…¼å®¹çš„ API æ ¼å¼ï¼‰
- âœ… å¤šæ¨¡æ€ï¼ˆå›¾åƒç†è§£ï¼Œæ”¯æŒ Visionï¼‰
- âœ… é•¿ä¸Šä¸‹æ–‡ï¼ˆæ”¯æŒ 128K+ tokensï¼‰
- âœ… LaTeX å…¬å¼ç”Ÿæˆï¼ˆæµ‹è¯•é€šè¿‡ï¼‰
- âŒ ä¸“ç”¨ OCR æ–‡æœ¬å®šä½ï¼ˆéœ€è¦æ›¿ä»£æ–¹æ¡ˆï¼‰

---

## 2. ä¸‰ç§æ–¹æ¡ˆå¯¹æ¯”

### æ–¹æ¡ˆ A: å…¨é‡æ›¿æ¢ï¼ˆæ¨èï¼‰

**æ–¹æ¡ˆæè¿°ï¼š**
- å°†æ‰€æœ‰ LLM è°ƒç”¨ï¼ˆGPT-4Vã€Mistralï¼‰æ›¿æ¢ä¸º Kimi API
- OCR éƒ¨åˆ†ä½¿ç”¨å¼€æºæ›¿ä»£ï¼ˆPaddleOCRï¼‰+ Kimi åå¤„ç†æ ¡éªŒ

**ä¼˜ç‚¹ï¼š**
- ç»Ÿä¸€ä½¿ç”¨ Kimi APIï¼Œç»´æŠ¤ç®€å•
- å®Œå…¨æ‘†è„± Azure/OpenAI/Mistral ä¾èµ–
- æˆæœ¬å¯æ§ï¼ˆKimi ä»·æ ¼ç›¸å¯¹è¾ƒä½ï¼‰

**ç¼ºç‚¹ï¼š**
- OCR éƒ¨åˆ†éœ€è¦é¢å¤–é›†æˆ PaddleOCR
- éœ€è¦æµ‹è¯• PaddleOCR çš„å‡†ç¡®ç‡

**å·¥ä½œé‡ï¼š**
- ä¿®æ”¹é…ç½®æ–‡ä»¶å’Œ API å®¢æˆ·ç«¯ä»£ç 
- é›†æˆ PaddleOCRï¼ˆçº¦ 2-3 å¤©ï¼‰
- æµ‹è¯•å’Œè°ƒä¼˜ï¼ˆçº¦ 1-2 å¤©ï¼‰

**é€‚ç”¨åœºæ™¯ï¼š**
- ä¸æƒ³ç”³è¯·å¤šä¸ª API Key
- å¸Œæœ›åœ¨æœ¬åœ°/ç§æœ‰ç¯å¢ƒè¿è¡Œ

---

### æ–¹æ¡ˆ B: æ··åˆæ¶æ„

**æ–¹æ¡ˆæè¿°ï¼š**
- LLM éƒ¨åˆ†å…¨éƒ¨ä½¿ç”¨ Kimi æ›¿ä»£
- OCR éƒ¨åˆ†ä¿ç•™ Azure Document Intelligence æˆ–ç”³è¯·å…è´¹é¢åº¦

**ä¼˜ç‚¹ï¼š**
- OCR å‡†ç¡®ç‡æœ‰ä¿éšœï¼ˆAzure DI ä¸“ä¸šçº§ï¼‰
- æ¶æ„æ¸…æ™°ï¼ŒèŒè´£åˆ†ç¦»

**ç¼ºç‚¹ï¼š**
- ä»éœ€ Azure API Key
- éœ€è¦ç®¡ç†å¤šä¸ªæœåŠ¡å•†

**å·¥ä½œé‡ï¼š**
- ä¿®æ”¹ LLM è°ƒç”¨ä»£ç ï¼ˆçº¦ 1 å¤©ï¼‰
- é…ç½® Azure OCRï¼ˆçº¦ 0.5 å¤©ï¼‰

**é€‚ç”¨åœºæ™¯ï¼š**
- å¯¹ OCR å‡†ç¡®ç‡è¦æ±‚æé«˜
- å¯ä»¥æ¥å—å¤šæœåŠ¡å•†ç®¡ç†

---

### æ–¹æ¡ˆ C: ç®€åŒ–ç‰ˆï¼ˆå¿«é€Ÿå¯åŠ¨ï¼‰

**æ–¹æ¡ˆæè¿°ï¼š**
- ä»…ä½¿ç”¨ Kimi è¿›è¡Œå›¾åƒæè¿°å’ŒåŸºç¡€åˆ†å‰²
- æš‚æ—¶ç¦ç”¨ OCR å’Œå…¬å¼è¯†åˆ«åŠŸèƒ½
- åç»­é€æ­¥æ·»åŠ 

**ä¼˜ç‚¹ï¼š**
- æœ€å¿«å¯åŠ¨ï¼ˆå½“å¤©å¯ç”¨ï¼‰
- ä»£ç æ”¹åŠ¨æœ€å°
- å¯ä»¥å¿«é€ŸéªŒè¯æ ¸å¿ƒæµç¨‹

**ç¼ºç‚¹ï¼š**
- åŠŸèƒ½ä¸å®Œæ•´ï¼ˆæ— æ–‡å­—æå–ï¼‰
- è¾“å‡ºè´¨é‡å—é™

**å·¥ä½œé‡ï¼š**
- ä¿®æ”¹é…ç½®ï¼ˆçº¦ 2 å°æ—¶ï¼‰
- ç¦ç”¨ text æ¨¡å—ï¼ˆçº¦ 1 å°æ—¶ï¼‰

**é€‚ç”¨åœºæ™¯ï¼š**
- å¿«é€ŸéªŒè¯é¡¹ç›®å¯è¡Œæ€§
- MVP æ¼”ç¤º

---

## 3. æ¨èçš„ä¿®å¤æ­¥éª¤ï¼ˆåˆ†ä¼˜å…ˆçº§ï¼‰

### ä¼˜å…ˆçº§ 1: ç«‹å³æ‰§è¡Œï¼ˆä»Šå¤©å®Œæˆï¼‰

1. **ä¿®æ”¹ .env é…ç½®**
   - æ·»åŠ  Kimi API é…ç½®
   - ä¿ç•™åŸæœ‰é…ç½®ï¼ˆå…¼å®¹å›é€€ï¼‰

2. **åˆ›å»ºç»Ÿä¸€ LLM å®¢æˆ·ç«¯**
   - å°è£… Kimi API è°ƒç”¨
   - å®ç°ä¸ç°æœ‰æ¥å£çš„é€‚é…å±‚

3. **æµ‹è¯• Kimi API è¿é€šæ€§**
   - éªŒè¯æ–‡æœ¬ç”ŸæˆåŠŸèƒ½
   - éªŒè¯å›¾åƒç†è§£åŠŸèƒ½

### ä¼˜å…ˆçº§ 2: çŸ­æœŸå®Œæˆï¼ˆæœ¬å‘¨å†…ï¼‰

4. **æ›¿æ¢ Mistral è°ƒç”¨**
   - å…¬å¼è¯†åˆ« â†’ Kimi
   - æ–‡æœ¬ä¿®æ­£ â†’ Kimi

5. **æ›¿æ¢ GPT-4V è°ƒç”¨**
   - å›¾åƒåˆ†æ â†’ Kimi Vision

6. **é›†æˆ PaddleOCRï¼ˆæ–¹æ¡ˆ Aï¼‰æˆ–ä¿ç•™ Azure OCRï¼ˆæ–¹æ¡ˆ Bï¼‰**

### ä¼˜å…ˆçº§ 3: ä¸­æœŸä¼˜åŒ–ï¼ˆä¸‹å‘¨ï¼‰

7. **å®Œå–„é”™è¯¯å¤„ç†å’Œé™çº§æœºåˆ¶**
8. **æ·»åŠ ç¼“å­˜å±‚å‡å°‘ API è°ƒç”¨**
9. **æ€§èƒ½ä¼˜åŒ–å’Œå¹¶å‘å¤„ç†**

---

## 4. ä»£ç ä¿®æ”¹ç¤ºä¾‹

### 4.1 é…ç½®æ–‡ä»¶ä¿®æ”¹ï¼ˆ.envï¼‰

**åŸé…ç½®ï¼š**
```bash
# Azure OpenAI
AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/
AZURE_OPENAI_KEY=your-azure-openai-key
AZURE_OPENAI_API_VERSION=2024-02-01
AZURE_OPENAI_DEPLOYMENT_NAME=gpt-4

# Mistral
MISTRAL_API_KEY=your-mistral-key
MISTRAL_MODEL=mistral-large-latest

# OpenAI
OPENAI_API_KEY=your-openai-key
OPENAI_MODEL=gpt-4
```

**æ–°é…ç½®ï¼ˆæ¨èï¼‰ï¼š**
```bash
# ============================================
# Kimi API é…ç½®ï¼ˆä¸»ç”¨ï¼‰
# ============================================
KIMI_BASE_URL=https://api.kimi.com/coding/
KIMI_API_KEY=sk-kimi-SckItPdArEsXNGKFoCYHMd8uG1FjpDnG8m1mEi1vQ6VMzhMhtVgFVqKilMthoVXN
KIMI_MODEL=kimi-v1

# ============================================
# LLM æä¾›å•†é€‰æ‹©
# ============================================
# å¯é€‰å€¼: kimi, azure, openai, mistral
LLM_PROVIDER=kimi

# ============================================
# OCR é…ç½®ï¼ˆæ–¹æ¡ˆ A: å¼€æºï¼‰
# ============================================
OCR_ENGINE=paddleocr  # å¯é€‰: paddleocr, azure, easyocr, none
PADDLEOCR_LANG=ch_sim,en

# ============================================
# Azure é…ç½®ï¼ˆæ–¹æ¡ˆ B: ä¿ç•™ï¼‰
# ============================================
AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/
AZURE_OPENAI_KEY=your-azure-openai-key
AZURE_OCR_ENDPOINT=https://your-ocr.cognitiveservices.azure.com/
AZURE_OCR_KEY=your-ocr-key

# ============================================
# å¤‡ç”¨é…ç½®ï¼ˆå¯é€‰ï¼‰
# ============================================
MISTRAL_API_KEY=your-mistral-key
OPENAI_API_KEY=your-openai-key
```

### 4.2 åˆ›å»ºç»Ÿä¸€ LLM å®¢æˆ·ç«¯

**æ–°æ–‡ä»¶ï¼š`modules/llm_client.py`**

```python
"""
ç»Ÿä¸€ LLM å®¢æˆ·ç«¯
æ”¯æŒ Kimiã€Azureã€OpenAIã€Mistral ç­‰å¤šç§åç«¯
"""

import os
import base64
from typing import Optional, Dict, Any, List
from abc import ABC, abstractmethod

# å°è¯•å¯¼å…¥å„ç§å®¢æˆ·ç«¯
try:
    import anthropic
    ANTHROPIC_AVAILABLE = True
except ImportError:
    ANTHROPIC_AVAILABLE = False

try:
    from openai import AzureOpenAI, OpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

try:
    import mistralai
    MISTRAL_AVAILABLE = True
except ImportError:
    MISTRAL_AVAILABLE = False


class BaseLLMClient(ABC):
    """LLM å®¢æˆ·ç«¯åŸºç±»"""
    
    @abstractmethod
    def chat(self, messages: List[Dict[str, str]], **kwargs) -> str:
        """å‘é€èŠå¤©è¯·æ±‚"""
        pass
    
    @abstractmethod
    def chat_with_image(self, messages: List[Dict[str, Any]], **kwargs) -> str:
        """å‘é€å¸¦å›¾ç‰‡çš„èŠå¤©è¯·æ±‚"""
        pass


class KimiClient(BaseLLMClient):
    """Kimi API å®¢æˆ·ç«¯ï¼ˆAnthropic æ ¼å¼ï¼‰"""
    
    def __init__(self, api_key: Optional[str] = None, base_url: Optional[str] = None):
        if not ANTHROPIC_AVAILABLE:
            raise ImportError("anthropic package required. Install: pip install anthropic")
        
        self.api_key = api_key or os.getenv("KIMI_API_KEY")
        self.base_url = base_url or os.getenv("KIMI_BASE_URL", "https://api.kimi.com/coding/")
        self.model = os.getenv("KIMI_MODEL", "kimi-v1")
        
        self.client = anthropic.Anthropic(
            api_key=self.api_key,
            base_url=self.base_url
        )
    
    def chat(self, messages: List[Dict[str, str]], **kwargs) -> str:
        """å‘é€çº¯æ–‡æœ¬èŠå¤©è¯·æ±‚"""
        # è½¬æ¢æ¶ˆæ¯æ ¼å¼ä¸º Anthropic æ ¼å¼
        system_msg = ""
        chat_messages = []
        
        for msg in messages:
            if msg.get("role") == "system":
                system_msg = msg.get("content", "")
            else:
                chat_messages.append({
                    "role": msg.get("role"),
                    "content": msg.get("content", "")
                })
        
        response = self.client.messages.create(
            model=self.model,
            max_tokens=kwargs.get("max_tokens", 4096),
            temperature=kwargs.get("temperature", 0.7),
            system=system_msg,
            messages=chat_messages
        )
        
        return response.content[0].text
    
    def chat_with_image(self, messages: List[Dict[str, Any]], **kwargs) -> str:
        """å‘é€å¸¦å›¾ç‰‡çš„èŠå¤©è¯·æ±‚"""
        system_msg = ""
        chat_messages = []
        
        for msg in messages:
            if msg.get("role") == "system":
                system_msg = msg.get("content", "")
            elif msg.get("role") == "user":
                content = msg.get("content", [])
                # å¤„ç†å¤šæ¨¡æ€å†…å®¹
                formatted_content = []
                for item in content:
                    if item.get("type") == "text":
                        formatted_content.append({
                            "type": "text",
                            "text": item.get("text", "")
                        })
                    elif item.get("type") == "image_url":
                        # å¤„ç†å›¾ç‰‡
                        image_url = item.get("image_url", {}).get("url", "")
                        if image_url.startswith("data:image"):
                            # base64 å›¾ç‰‡
                            import re
                            match = re.match(r'data:image/[^;]+;base64,(.+)', image_url)
                            if match:
                                base64_data = match.group(1)
                                formatted_content.append({
                                    "type": "image",
                                    "source": {
                                        "type": "base64",
                                        "media_type": "image/png",
                                        "data": base64_data
                                    }
                                })
                
                chat_messages.append({
                    "role": "user",
                    "content": formatted_content
                })
        
        response = self.client.messages.create(
            model=self.model,
            max_tokens=kwargs.get("max_tokens", 4096),
            temperature=kwargs.get("temperature", 0.7),
            system=system_msg,
            messages=chat_messages
        )
        
        return response.content[0].text


class AzureOpenAIClient(BaseLLMClient):
    """Azure OpenAI å®¢æˆ·ç«¯"""
    
    def __init__(self):
        if not OPENAI_AVAILABLE:
            raise ImportError("openai package required. Install: pip install openai")
        
        self.client = AzureOpenAI(
            api_key=os.getenv("AZURE_OPENAI_KEY"),
            api_version=os.getenv("AZURE_OPENAI_API_VERSION", "2024-02-01"),
            azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT")
        )
        self.deployment = os.getenv("AZURE_OPENAI_DEPLOYMENT_NAME", "gpt-4")
    
    def chat(self, messages: List[Dict[str, str]], **kwargs) -> str:
        response = self.client.chat.completions.create(
            model=self.deployment,
            messages=messages,
            max_tokens=kwargs.get("max_tokens", 4096),
            temperature=kwargs.get("temperature", 0.7)
        )
        return response.choices[0].message.content
    
    def chat_with_image(self, messages: List[Dict[str, Any]], **kwargs) -> str:
        # Azure ä¹Ÿä½¿ç”¨ç›¸åŒçš„ chat æ¥å£ï¼Œmessages ä¸­åŒ…å« image_url
        return self.chat(messages, **kwargs)


class LLMClientFactory:
    """LLM å®¢æˆ·ç«¯å·¥å‚"""
    
    _clients: Dict[str, BaseLLMClient] = {}
    
    @classmethod
    def get_client(cls, provider: Optional[str] = None) -> BaseLLMClient:
        """è·å– LLM å®¢æˆ·ç«¯å®ä¾‹"""
        provider = provider or os.getenv("LLM_PROVIDER", "kimi")
        
        if provider not in cls._clients:
            if provider == "kimi":
                cls._clients[provider] = KimiClient()
            elif provider == "azure":
                cls._clients[provider] = AzureOpenAIClient()
            elif provider == "openai":
                # å¯æ‰©å±•
                raise NotImplementedError("OpenAI client not implemented yet")
            elif provider == "mistral":
                raise NotImplementedError("Mistral client not implemented yet")
            else:
                raise ValueError(f"Unknown LLM provider: {provider}")
        
        return cls._clients[provider]
    
    @classmethod
    def reset(cls):
        """é‡ç½®æ‰€æœ‰å®¢æˆ·ç«¯ï¼ˆç”¨äºæµ‹è¯•ï¼‰"""
        cls._clients = {}


# ä¾¿æ·å‡½æ•°
def chat(messages: List[Dict[str, str]], **kwargs) -> str:
    """ä½¿ç”¨é»˜è®¤ provider å‘é€èŠå¤©è¯·æ±‚"""
    client = LLMClientFactory.get_client()
    return client.chat(messages, **kwargs)


def chat_with_image(messages: List[Dict[str, Any]], **kwargs) -> str:
    """ä½¿ç”¨é»˜è®¤ provider å‘é€å¸¦å›¾ç‰‡çš„èŠå¤©è¯·æ±‚"""
    client = LLMClientFactory.get_client()
    return client.chat_with_image(messages, **kwargs)
```

### 4.3 OCR å®¢æˆ·ç«¯ï¼ˆPaddleOCR æ–¹æ¡ˆï¼‰

**æ–°æ–‡ä»¶ï¼š`modules/ocr_client.py`**

```python
"""
ç»Ÿä¸€ OCR å®¢æˆ·ç«¯
æ”¯æŒ PaddleOCRã€Azureã€EasyOCR ç­‰å¤šç§åç«¯
"""

import os
from typing import Optional, List, Dict, Any
from abc import ABC, abstractmethod
import numpy as np


class BaseOCRClient(ABC):
    """OCR å®¢æˆ·ç«¯åŸºç±»"""
    
    @abstractmethod
    def recognize(self, image: np.ndarray) -> List[Dict[str, Any]]:
        """
        è¯†åˆ«å›¾åƒä¸­çš„æ–‡å­—
        
        Returns:
            List[{
                "text": str,
                "confidence": float,
                "bbox": {"x": int, "y": int, "width": int, "height": int}
            }]
        """
        pass


class PaddleOCRClient(BaseOCRClient):
    """PaddleOCR å®¢æˆ·ç«¯"""
    
    def __init__(self, lang: str = "ch_sim,en"):
        try:
            from paddleocr import PaddleOCR
        except ImportError:
            raise ImportError("paddleocr required. Install: pip install paddleocr")
        
        self.ocr = PaddleOCR(
            use_angle_cls=True,
            lang=lang,
            show_log=False
        )
    
    def recognize(self, image: np.ndarray) -> List[Dict[str, Any]]:
        """ä½¿ç”¨ PaddleOCR è¯†åˆ«æ–‡å­—"""
        result = self.ocr.ocr(image, cls=True)
        
        texts = []
        if result and result[0]:
            for line in result[0]:
                if line:
                    bbox = line[0]  # å››ä¸ªè§’ç‚¹åæ ‡
                    text = line[1][0]  # æ–‡å­—å†…å®¹
                    confidence = line[1][1]  # ç½®ä¿¡åº¦
                    
                    # è®¡ç®—çŸ©å½¢æ¡†
                    xs = [p[0] for p in bbox]
                    ys = [p[1] for p in bbox]
                    x_min, x_max = min(xs), max(xs)
                    y_min, y_max = min(ys), max(ys)
                    
                    texts.append({
                        "text": text,
                        "confidence": confidence,
                        "bbox": {
                            "x": int(x_min),
                            "y": int(y_min),
                            "width": int(x_max - x_min),
                            "height": int(y_max - y_min)
                        }
                    })
        
        return texts


class AzureOCRClient(BaseOCRClient):
    """Azure Document Intelligence å®¢æˆ·ç«¯"""
    
    def __init__(self):
        self.endpoint = os.getenv("AZURE_OCR_ENDPOINT")
        self.key = os.getenv("AZURE_OCR_KEY")
        
        if not self.endpoint or not self.key:
            raise ValueError("Azure OCR credentials not configured")
    
    def recognize(self, image: np.ndarray) -> List[Dict[str, Any]]:
        """ä½¿ç”¨ Azure DI è¯†åˆ«æ–‡å­—"""
        # Azure DI å®ç°
        # å‚è€ƒ: https://docs.microsoft.com/azure/ai-services/document-intelligence/
        raise NotImplementedError("Azure OCR implementation pending")


class OCRClientFactory:
    """OCR å®¢æˆ·ç«¯å·¥å‚"""
    
    _clients: Dict[str, BaseOCRClient] = {}
    
    @classmethod
    def get_client(cls, engine: Optional[str] = None) -> Optional[BaseOCRClient]:
        """è·å– OCR å®¢æˆ·ç«¯å®ä¾‹"""
        engine = engine or os.getenv("OCR_ENGINE", "paddleocr")
        
        if engine == "none":
            return None
        
        if engine not in cls._clients:
            if engine == "paddleocr":
                lang = os.getenv("PADDLEOCR_LANG", "ch_sim,en")
                cls._clients[engine] = PaddleOCRClient(lang=lang)
            elif engine == "azure":
                cls._clients[engine] = AzureOCRClient()
            elif engine == "easyocr":
                raise NotImplementedError("EasyOCR not implemented yet")
            else:
                raise ValueError(f"Unknown OCR engine: {engine}")
        
        return cls._clients[engine]
```

### 4.4 TextRestorer ä¿®æ”¹

**ä¿®æ”¹ï¼š`modules/text/text_render.py`**

```python
"""
Text Render Module
æ–‡å­—æ¸²æŸ“å’Œæ¢å¤ - æ”¯æŒå¤šç§ LLM å’Œ OCR åç«¯
"""

import os
from typing import Optional, Dict, Any, List
import numpy as np
from PIL import Image, ImageDraw, ImageFont

# å¯¼å…¥ç»Ÿä¸€å®¢æˆ·ç«¯
try:
    from modules.llm_client import chat, chat_with_image, LLMClientFactory
    from modules.ocr_client import OCRClientFactory
    CLIENTS_AVAILABLE = True
except ImportError:
    CLIENTS_AVAILABLE = False


class TextRestorer:
    """æ–‡å­—æ¢å¤å™¨ - æ”¯æŒ Kimi/Azure/OpenAI/Mistral"""
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        self.default_font_size = self.config.get("default_font_size", 14)
        self.default_font_family = self.config.get("default_font_family", "Arial")
        self.formula_engine = self.config.get("formula_engine", "kimi")  # kimi / none
        
        # åˆå§‹åŒ–å®¢æˆ·ç«¯
        self._ocr_client = None
        self._llm_client = None
        
        if CLIENTS_AVAILABLE:
            try:
                self._ocr_client = OCRClientFactory.get_client()
            except Exception as e:
                print(f"OCR client initialization failed: {e}")
            
            try:
                self._llm_client = LLMClientFactory.get_client()
            except Exception as e:
                print(f"LLM client initialization failed: {e}")
    
    def process(self, image_path: str) -> str:
        """
        å¤„ç†å›¾åƒï¼Œæå–æ–‡å­—å¹¶ç”Ÿæˆ XML
        
        Args:
            image_path: è¾“å…¥å›¾åƒè·¯å¾„
            
        Returns:
            str: DrawIO XML æ ¼å¼å­—ç¬¦ä¸²
        """
        import cv2
        image = cv2.imread(image_path)
        if image is None:
            raise ValueError(f"Cannot load image: {image_path}")
        
        # 1. OCR æ£€æµ‹æ–‡å­—åŒºåŸŸ
        text_regions = self._detect_text_regions(image)
        
        # 2. å¯¹æ¯ä¸ªåŒºåŸŸè¿›è¡Œè¯¦ç»†è¯†åˆ«
        recognized_texts = []
        for region in text_regions:
            text_info = self._recognize_text_detail(image, region)
            recognized_texts.append(text_info)
        
        # 3. ç”Ÿæˆ XML
        xml_content = self._generate_xml(recognized_texts)
        return xml_content
    
    def _detect_text_regions(self, image: np.ndarray) -> List[Dict[str, Any]]:
        """æ£€æµ‹æ–‡å­—åŒºåŸŸ"""
        if self._ocr_client:
            return self._ocr_client.recognize(image)
        return []
    
    def _recognize_text_detail(self, image: np.ndarray, region: Dict[str, Any]) -> Dict[str, Any]:
        """è¯¦ç»†è¯†åˆ«æ–‡å­—å†…å®¹ï¼ˆåŒ…æ‹¬å…¬å¼ï¼‰"""
        bbox = region.get("bbox", {})
        x, y = bbox.get("x", 0), bbox.get("y", 0)
        w, h = bbox.get("width", 0), bbox.get("height", 0)
        
        # è£å‰ªåŒºåŸŸ
        crop = image[y:y+h, x:x+w]
        
        # åŸºç¡€æ–‡å­—
        text = region.get("text", "")
        
        # å¦‚æœæ˜¯å…¬å¼æ¨¡å¼ï¼Œä½¿ç”¨ LLM è¿›ä¸€æ­¥è¯†åˆ«
        if self.formula_engine != "none" and self._llm_client:
            text = self._recognize_formula(crop, text)
        
        return {
            "text": text,
            "bbox": bbox,
            "confidence": region.get("confidence", 0),
            "is_formula": self._is_formula(text)
        }
    
    def _recognize_formula(self, image_crop: np.ndarray, hint_text: str) -> str:
        """ä½¿ç”¨ LLM è¯†åˆ«å…¬å¼"""
        try:
            # å°†å›¾ç‰‡è½¬ä¸º base64
            import cv2
            from PIL import Image
            import io
            import base64
            
            # è½¬æ¢é¢œè‰²ç©ºé—´
            if len(image_crop.shape) == 3:
                image_crop = cv2.cvtColor(image_crop, cv2.COLOR_BGR2RGB)
            
            pil_image = Image.fromarray(image_crop)
            buffer = io.BytesIO()
            pil_image.save(buffer, format="PNG")
            img_base64 = base64.b64encode(buffer.getvalue()).decode()
            
            # æ„å»ºæç¤º
            prompt = f"""è¯†åˆ«å›¾ç‰‡ä¸­çš„æ•°å­¦å…¬å¼æˆ–æ–‡å­—ã€‚
å¦‚æœåŒ…å«æ•°å­¦å…¬å¼ï¼Œè¯·è½¬æ¢ä¸º LaTeX æ ¼å¼ï¼ˆä½¿ç”¨ $ åŒ…è£¹ï¼‰ã€‚
å¦‚æœåªæœ‰æ™®é€šæ–‡å­—ï¼Œç›´æ¥è¿”å›æ–‡å­—å†…å®¹ã€‚
OCR æç¤º: {hint_text}

åªè¿”å›è¯†åˆ«ç»“æœï¼Œä¸è¦è§£é‡Šã€‚"""
            
            messages = [
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": prompt},
                        {"type": "image_url", "image_url": {"url": f"data:image/png;base64,{img_base64}"}}
                    ]
                }
            ]
            
            result = self._llm_client.chat_with_image(messages, temperature=0.3)
            return result.strip()
            
        except Exception as e:
            print(f"Formula recognition failed: {e}")
            return hint_text
    
    def _is_formula(self, text: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºå…¬å¼"""
        formula_indicators = ['$', '\\', '^', '_', '{', '}', 'frac', 'sum', 'int', 'sqrt']
        return any(indicator in text for indicator in formula_indicators)
    
    def _generate_xml(self, texts: List[Dict[str, Any]]) -> str:
        """ç”Ÿæˆ DrawIO XML"""
        # ç®€åŒ–ç‰ˆ XML ç”Ÿæˆ
        xml_parts = [
            '<?xml version="1.0" encoding="UTF-8"?>',
            '<mxfile version="21.0">',
            '<diagram name="Page-1">',
            '<mxGraphModel dx="800" dy="600" grid="1">',
            '<root>',
            '<mxCell id="0" />',
            '<mxCell id="1" parent="0" />'
        ]
        
        for i, text_info in enumerate(texts, start=2):
            bbox = text_info.get("bbox", {})
            x, y = bbox.get("x", 0), bbox.get("y", 0)
            w, h = bbox.get("width", 100), bbox.get("height", 20)
            text = text_info.get("text", "")
            
            # è½¬ä¹‰ç‰¹æ®Šå­—ç¬¦
            text_escaped = text.replace('&', '&amp;').replace('<', '&lt;').replace('>', '&gt;').replace('"', '&quot;')
            
            xml_parts.append(
                f'<mxCell id="{i}" value="{text_escaped}" style="text;html=1;" '
                f'vertex="1" parent="1">'
                f'<mxGeometry x="{x}" y="{y}" width="{w}" height="{h}" as="geometry" />'
                f'</mxCell>'
            )
        
        xml_parts.extend([
            '</root>',
            '</mxGraphModel>',
            '</diagram>',
            '</mxfile>'
        ])
        
        return '\n'.join(xml_parts)
    
    def restore_text(self, image: np.ndarray, text_info: Dict[str, Any]) -> np.ndarray:
        """åœ¨å›¾åƒä¸Šæ¢å¤æ–‡å­—ï¼ˆåŸæœ‰åŠŸèƒ½ä¿ç•™ï¼‰"""
        if len(image.shape) == 2:
            pil_image = Image.fromarray(image).convert('RGB')
        else:
            pil_image = Image.fromarray(image)
        
        draw = ImageDraw.Draw(pil_image)
        text = text_info.get("text", "")
        bbox = text_info.get("bbox", {"x": 0, "y": 0, "width": 100, "height": 20})
        
        x = int(bbox.get("x", 0))
        y = int(bbox.get("y", 0))
        
        try:
            font = ImageFont.truetype(self.default_font_family, self.default_font_size)
        except:
            font = ImageFont.load_default()
        
        draw.text((x, y), text, fill=(0, 0, 0), font=font)
        return np.array(pil_image)
```

---

## 5. é…ç½®å»ºè®®

### 5.1 ç¯å¢ƒå˜é‡ (.env)

```bash
# ============================================
# ä¸»é…ç½®ï¼šKimi API
# ============================================
KIMI_BASE_URL=https://api.kimi.com/coding/
KIMI_API_KEY=sk-kimi-SckItPdArEsXNGKFoCYHMd8uG1FjpDnG8m1mEi1vQ6VMzhMhtVgFVqKilMthoVXN
KIMI_MODEL=kimi-v1

# ============================================
# æä¾›å•†é€‰æ‹©
# ============================================
LLM_PROVIDER=kimi           # ä¸» LLM: kimi / azure / openai / mistral
OCR_ENGINE=paddleocr        # OCR: paddleocr / azure / easyocr / none

# ============================================
# OCR é…ç½®
# ============================================
PADDLEOCR_LANG=ch_sim,en    # PaddleOCR è¯­è¨€åŒ…

# ============================================
# å¤‡ç”¨é…ç½®ï¼ˆå¯é€‰ï¼‰
# ============================================
# Azure OpenAIï¼ˆå¦‚éœ€è¦ï¼‰
AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/
AZURE_OPENAI_KEY=your-azure-openai-key
AZURE_OPENAI_API_VERSION=2024-02-01
AZURE_OPENAI_DEPLOYMENT_NAME=gpt-4

# Azure OCRï¼ˆå¦‚éœ€è¦ï¼‰
AZURE_OCR_ENDPOINT=https://your-ocr.cognitiveservices.azure.com/
AZURE_OCR_KEY=your-ocr-key

# Mistralï¼ˆå¦‚éœ€è¦ï¼‰
MISTRAL_API_KEY=your-mistral-key
MISTRAL_MODEL=mistral-large-latest

# OpenAIï¼ˆå¦‚éœ€è¦ï¼‰
OPENAI_API_KEY=your-openai-key
OPENAI_MODEL=gpt-4

# ============================================
# æ¨¡å‹è·¯å¾„
# ============================================
SAM3_CHECKPOINT_PATH=models/sam3_checkpoint.pth

# ============================================
# åº”ç”¨é…ç½®
# ============================================
APP_DEBUG=false
APP_HOST=0.0.0.0
APP_PORT=8000
LOG_LEVEL=INFO
```

### 5.2 ä¾èµ–å®‰è£… (requirements.txt)

```txt
# åŸºç¡€ä¾èµ–
fastapi>=0.104.0
uvicorn>=0.24.0
pydantic>=2.0.0
python-dotenv>=1.0.0
pyyaml>=6.0

# å›¾åƒå¤„ç†
numpy>=1.24.0
opencv-python>=4.8.0
pillow>=10.0.0

# æ·±åº¦å­¦ä¹ 
torch>=2.0.0
torchvision>=0.15.0
transformers>=4.35.0
accelerate>=0.24.0

# LLM å®¢æˆ·ç«¯ï¼ˆå¿…é€‰å…¶ä¸€ï¼‰
anthropic>=0.8.0          # Kimi APIï¼ˆAnthropic æ ¼å¼ï¼‰
# openai>=1.0.0           # Azure/OpenAIï¼ˆå¯é€‰ï¼‰
# mistralai>=0.0.8        # Mistralï¼ˆå¯é€‰ï¼‰

# OCR å¼•æ“ï¼ˆæ ¹æ®é…ç½®é€‰æ‹©ï¼‰
paddleocr>=2.7.0          # æ¨èä¸­æ–‡ OCR
paddlepaddle>=2.5.0       # Paddle åŸºç¡€åº“
# easyocr>=1.7.0          # å¤‡é€‰ OCR

# å¯é€‰ä¾èµ–
spandrel>=0.1.0           # è¶…åˆ†æ¨¡å‹
python-pptx>=0.6.21       # PPTX ç”Ÿæˆ
```

### 5.3 å®‰è£…è„šæœ¬

```bash
#!/bin/bash
# setup.sh - å¿«é€Ÿå®‰è£…è„šæœ¬

echo "ğŸš€ Edit-Banana å®‰è£…è„šæœ¬"

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python3 -m venv venv
source venv/bin/activate

# å‡çº§ pip
pip install --upgrade pip

# å®‰è£…åŸºç¡€ä¾èµ–
echo "ğŸ“¦ å®‰è£…åŸºç¡€ä¾èµ–..."
pip install fastapi uvicorn pydantic python-dotenv pyyaml
pip install numpy opencv-python pillow

# å®‰è£…æ·±åº¦å­¦ä¹ æ¡†æ¶
echo "ğŸ“¦ å®‰è£… PyTorch..."
pip install torch torchvision

# å®‰è£… LLM å®¢æˆ·ç«¯
echo "ğŸ“¦ å®‰è£… LLM å®¢æˆ·ç«¯..."
pip install anthropic  # Kimi

# å®‰è£… OCR å¼•æ“
echo "ğŸ“¦ å®‰è£… PaddleOCR..."
pip install paddlepaddle paddleocr

# å¯é€‰ä¾èµ–
echo "ğŸ“¦ å®‰è£…å¯é€‰ä¾èµ–..."
pip install transformers accelerate

echo "âœ… å®‰è£…å®Œæˆï¼"
echo ""
echo "ä¸‹ä¸€æ­¥ï¼š"
echo "1. å¤åˆ¶ .env.example ä¸º .env"
echo "2. å¡«å…¥ä½ çš„ API Keys"
echo "3. è¿è¡Œ: python server_pa.py"
```

---

## 6. æµ‹è¯•éªŒè¯æ¸…å•

### 6.1 å•å…ƒæµ‹è¯•

```python
# tests/test_llm_client.py
import pytest
from modules.llm_client import KimiClient, LLMClientFactory

def test_kimi_client_init():
    """æµ‹è¯• Kimi å®¢æˆ·ç«¯åˆå§‹åŒ–"""
    client = KimiClient()
    assert client.api_key is not None
    assert client.base_url is not None

def test_kimi_chat():
    """æµ‹è¯• Kimi èŠå¤©åŠŸèƒ½"""
    client = KimiClient()
    messages = [
        {"role": "user", "content": "Hello, this is a test."}
    ]
    response = client.chat(messages, max_tokens=50)
    assert len(response) > 0

def test_kimi_vision():
    """æµ‹è¯• Kimi å›¾åƒç†è§£"""
    client = KimiClient()
    # ä½¿ç”¨ base64 æµ‹è¯•å›¾ç‰‡
    messages = [
        {
            "role": "user",
            "content": [
                {"type": "text", "text": "What is this image?"},
                {"type": "image_url", "image_url": {"url": "data:image/png;base64,..."}}
            ]
        }
    ]
    response = client.chat_with_image(messages, max_tokens=100)
    assert len(response) > 0
```

### 6.2 é›†æˆæµ‹è¯•

```python
# tests/test_text_restorer.py
import pytest
import numpy as np
from modules.text.text_render import TextRestorer

def test_text_restorer_init():
    """æµ‹è¯• TextRestorer åˆå§‹åŒ–"""
    restorer = TextRestorer()
    assert restorer.default_font_size == 14

def test_text_restorer_process():
    """æµ‹è¯•æ–‡å­—æå–æµç¨‹"""
    restorer = TextRestorer()
    # åˆ›å»ºæµ‹è¯•å›¾åƒ
    image = np.ones((100, 200, 3), dtype=np.uint8) * 255
    # æµ‹è¯•å¤„ç†ï¼ˆéœ€è¦çœŸå®å›¾åƒæ–‡ä»¶ï¼‰
    # xml = restorer.process("test_image.png")
    # assert "mxfile" in xml
```

---

## 7. å¸¸è§é—®é¢˜ FAQ

### Q1: Kimi API æ˜¯å¦æ”¯æŒ LaTeX å…¬å¼ï¼Ÿ
**A:** âœ… æ”¯æŒã€‚Kimi å¯ä»¥ç†è§£å’Œç”Ÿæˆ LaTeX æ ¼å¼çš„æ•°å­¦å…¬å¼ã€‚

### Q2: PaddleOCR çš„ä¸­æ–‡è¯†åˆ«å‡†ç¡®ç‡å¦‚ä½•ï¼Ÿ
**A:** PaddleOCR æ˜¯å¼€æº OCR ä¸­ä¸­æ–‡è¯†åˆ«æ•ˆæœæœ€å¥½çš„ä¹‹ä¸€ï¼Œæ”¯æŒä¸­è‹±æ–‡æ··åˆè¯†åˆ«ï¼Œå‡†ç¡®ç‡æ¥è¿‘å•†ä¸š APIã€‚

### Q3: å¦‚æœä¸æƒ³ç”¨ PaddleOCRï¼Œè¿˜æœ‰ä»€ä¹ˆé€‰æ‹©ï¼Ÿ
**A:** å¯é€‰æ–¹æ¡ˆï¼š
- EasyOCRï¼ˆå¤šè¯­è¨€æ”¯æŒå¥½ï¼‰
- Tesseractï¼ˆè€ç‰Œå¼€æº OCRï¼‰
- é˜¿é‡Œäº‘/è…¾è®¯äº‘ OCRï¼ˆå›½å†… APIï¼‰

### Q4: Kimi API çš„è°ƒç”¨é™åˆ¶æ˜¯ä»€ä¹ˆï¼Ÿ
**A:** è¯·å‚è€ƒ Moonshot AI å®˜æ–¹æ–‡æ¡£è·å–æœ€æ–°çš„é€Ÿç‡é™åˆ¶ä¿¡æ¯ã€‚

### Q5: å¦‚ä½•å›é€€åˆ°åŸæ¥çš„ Azure/OpenAIï¼Ÿ
**A:** åªéœ€ä¿®æ”¹ `.env` ä¸­çš„ `LLM_PROVIDER` å’Œ `OCR_ENGINE` é…ç½®å³å¯æ— ç¼åˆ‡æ¢ã€‚

---

## 8. æ€»ç»“ä¸å»ºè®®

### æ¨èæ–¹æ¡ˆï¼šæ–¹æ¡ˆ Aï¼ˆå…¨é‡æ›¿æ¢ï¼‰

**ç†ç”±ï¼š**
1. å®Œå…¨æ‘†è„±å¯¹ Azure/OpenAI/Mistral çš„ä¾èµ–
2. Kimi åœ¨ä¸­æ–‡åœºæ™¯ä¸‹è¡¨ç°ä¼˜å¼‚
3. PaddleOCR å¼€æºå…è´¹ï¼Œå‡†ç¡®ç‡å¯æ¥å—
4. ç»Ÿä¸€æŠ€æœ¯æ ˆï¼Œç»´æŠ¤ç®€å•

### å®æ–½æ—¶é—´çº¿

| é˜¶æ®µ | ä»»åŠ¡ | é¢„è®¡æ—¶é—´ |
|------|------|---------|
| Day 1 | åˆ›å»º LLM/OCR å®¢æˆ·ç«¯ | 4 å°æ—¶ |
| Day 2 | ä¿®æ”¹ TextRestorer | 4 å°æ—¶ |
| Day 3 | é›†æˆæµ‹è¯• | 4 å°æ—¶ |
| Day 4 | Bug ä¿®å¤å’Œä¼˜åŒ– | 4 å°æ—¶ |

### é£é™©æç¤º

1. **PaddleOCR é¦–æ¬¡è¿è¡Œä¼šè‡ªåŠ¨ä¸‹è½½æ¨¡å‹**ï¼ˆçº¦ 100MBï¼‰ï¼Œéœ€è¦ç½‘ç»œè¿æ¥
2. **Kimi API å¯èƒ½éœ€è¦ç”³è¯·å†…æµ‹/æ­£å¼è´¦å·**
3. **OCR å‡†ç¡®ç‡å¯èƒ½ç•¥ä½äº Azure DI**ï¼Œéœ€è¦é’ˆå¯¹å…·ä½“åœºæ™¯è°ƒä¼˜

### ä¸‹ä¸€æ­¥è¡ŒåŠ¨

ç­‰å¾… Team Lead ç¡®è®¤æ–¹æ¡ˆåï¼Œå¼€å§‹å®æ–½ä»£ç ä¿®æ”¹ã€‚

---

**æŠ¥å‘Šç”Ÿæˆæ—¶é—´:** 2026-02-10
**æŠ¥å‘Šä½œè€…:** API Fix Suggestion Agent
**çŠ¶æ€:** å¾…ç¡®è®¤
